<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sathyaprakash Narayanan</title>
  
  <meta name="author" content="Sathyaprakash Narayanan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/x-icon" href="/images/icon-48x48.png">
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-8QMS4PNR02"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-8QMS4PNR02');
</script>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sathyaprakash Narayanan</name>
              </p>
              <p>Hi, There. I am a Computer Vision Researcher interested in Deep Learning, Neuromorphic Engineering and Reinforcement Learning. 
              </p>
              
              <p>
                I have had the privilige to work as <b>Machine Learning Scientist II </b>at <a href="https://www.lytx.com/en-us/">Lytx Inc.</a>
                Prior to that I worked as a <b>Research Associate</b> at <a href="https://labs.dese.iisc.ac.in/neuronics/">NeuRonICS Lab, DESE; Indian Institute of Science, Bangalore</a> and as a <b>Teaching Assistant</b> at <a href="https://www.mygreatlearning.com/">Great Learning, Bangalore.</a>
                <br>
                <br>
                Proud owner of my first pet, a python <a href="https://github.com/satabios/sconce">SCONCE (AutoDL compression and Deployment)</a> package.

              </p>
              <p>
                Fast-forward to today, I'm a ECE Graudate Student at <a href="https://www.ucsc.edu/">University of California, Santa Cruz.</a> Under by Professor and Mentor <a href="https://www.jasoneshraghian.com/">Jason Eshraghian</a> at UCSC. <br>
                <br>

                <br>
                Teaching at UCSC:
                <ul>
                  <li>[TIM170]: Management of Technology Seminar - Fall 2024</li>
                  <li>[ECE103]: Singal and Systems - Spring 2023</li>
                  <li>[CSE30]: Programming Abstractions in Python - Winter 2023</li>
             
                </ul>

                <br>
     
              </p>
              <p style="text-align:center">
                <a href="mailto:1f681as95@mozmail.com">Email</a>&nbsp/&nbsp
                <a href="data/resume.pdf">CV</a>&nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=rI5VHWoAAAAJ">Google Scholar</a>&nbsp/&nbsp
                <a href="https://github.com/satabios/">Github</a>&nbsp/&nbsp
                <a href="https://www.kaggle.com/satabios">Kaggle</a>&nbsp/&nbsp                
                <a href="https://twitter.com/sathyabratrat">Twitter</a>
                
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/IMG_4150.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/IMG_4150.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Interest</heading>
              <p>
                My research interests broadly are and but is not restricted to:
                <ul><li>Computer Vision</li>
                <li>Deep Learning</li>
                <li>Machine Learning</li>
                <li>Efficient ML/DL Model Hardware Deployment</li>
                <li>Computational Imaging</li>
                <li>Signal Processing</li>
                <li>Compressive Sensing</li>
                <li>Neuromorphic Engineering</li>
                <li>Event Based Data</li>
                </ul>
              
              <!-- <span class="highlight">highlighted</span>. -->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
          
          <hr>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
                <heading>Recent Update</heading>
            </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <ul>
          <li><strong>[Jun, 2023]</strong> Selected as a Reviewer for <b>WACV 2024</b></li>
          <li><strong>[May, 2023]</strong> Selected as a <b>SIP 2023 Mentor</b></li>
          <li><strong>[Apr, 2023]</strong> Pushed the first version of <b>Model Compression(tomoco)</b> </li>
          <li><strong>[Jan, 2023]</strong> Selected as a Reviewer for <b>WACV 2023</b></li>
          <li><strong>[Spring 2023]</strong> <b>ECE103</b>: Teaching Assistant for Signals and Systems Coursework at UCSC</li>
          <li><strong>[Winter 2023]</strong> <b>CSE30</b>: Teaching Assistant for  Programming Abstractions: Python at UCSC</li>
          <li><strong>[Dec, 2021]</strong> <b>Patent Published</b>: <i>METHOD AND SYSTEM FOR RECOGNIZING ACTIVITIES IN SURROUNDING ENVIRONMENT FOR CONTROLLING NAVIGATION OF AUTONOMOUS VEHICLE</i></li>
          <li><strong>[June, 2021]</strong> Paper Selected for <b>ICIP 2021</b></li>
          
          
        </ul>
      </tr>
      </tbody></table>


          <hr>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Paper</heading>
            </td>
          </tr>
          <!-- <h2 class="card-subheading">Paper</h2> -->
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        

          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/compressive_sensing_DL.png' height="120" width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
              
                    <b>Real-Time Object Detection and Localization in Compressive Sensed Video</b>
                
                    <i><strong>Sathyaprakash Narayanan</strong>*,
                      Yeshwanth Ravi Theja*,
                      Venkat Rangan,
                      Anirban Charkraborty,
                      Chetan Singh Thakur</i><br>
                <i>IEEE International Conference on Image Processing <b>(ICIP), 2021</b></i>
                <br>
                <div class="paper" id="btp">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9506769">Paper</a>/ 
                  <a href="https://labs.dese.iisc.ac.in/neuronics/datasets/">Dataset</a> 
                  
              </div>
                </p>
                <p>
                  A novel task of detection and localization of objects directly
                  on the compressed frames. Thereby mitigating the need to
                  reconstruct the frames and reducing the search rate up to
                  20x (compression rate). We achieved an accuracy of 46.27%
                  mAP with the proposed model. We were also able to show real-time inference on an NVIDIA
                  TX2 embedded board with 45.11% mAP.
                </p>
              
                <br>
            </td>
        </tr>

          <!-- <tr bgcolor="#ffffd0"> -->
        <td style="padding:20px;width:35%;vertical-align:middle">
            <img src='images/n-har.png' width="250"></div>
                </td>
        <td width="75%" valign="middle">
            <p>
           
                <b>n-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory Surfaces</b><br>
                <i>Bibrat Ranjan Pradhan,
                  Yeshwanth Ravi Theja,
                  <b>Sathyaprakash Narayanan</b>,
                  Anirban Charkraborty,
                  Chetan Singh Thakur</i><br>
                  <i>IEEE International Symposium on Circuits and Systems <b>(ISCAS), 2019</b></i>
                
            </a>
            

            </p>
            <div class="paper" id="btp">
              <a href="https://ieeexplore.ieee.org/abstract/document/8702581">Paper</a>/
              <a href="https://labs.dese.iisc.ac.in/neuronics/datasets/event-based-neuromorphic-datasets/">Code-Dataset</a>
              <br>
          </div>
            <p>
              A system to achieve the task of human activity recognition based on the event-based camera data. 
              We show that such tasks, which generally need high frame rate sensors for accurate predictions, can be achieved by adapting existing computer vision techniques to the spiking domain.
               We used event memory surfaces to make the sparse event data compatible with deep convolutional neural networks (CNNs).
                <!-- We leverage upon the recent advances in deep convolutional networks based video analysis and adapt such frameworks onto the neuromorphic domain. -->
                 We also provide the community with a new dataset consisting of five categories of human activities captured in real world without any simulations.
                  We achieved an accuracy of 94.3% using event memory surfaces on our activity recognition dataset.
            </p>
            
            <br>
        </td>
    </tr>

  

        <!-- <tr bgcolor="#ffffd0"> -->
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/saliency.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <!-- <p><strong>Paper: </strong> -->
                
                    <b>Real-Time Implementation of Proto-Object Based Visual Saliency Model</b><br>
                    <i><b>Sathyaprakash Narayanan</b>,
                      Yeshwanth Ravi Theja,
                      Jamal Lottier,
                      Ernst Niebur,
                      Ralph Etienne-Cummings,
                      Chetan Singh Thakur</i> <br>
                <i>IEEE International Symposium on Circuits and Systems <b>(ISCAS), 2019</b></i>
                </p>

                <div class="paper" id="ijcnn_2021">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8702200">Paper</a> 
                  <!-- <a href="https://github.com/raja-kumar/Hybrid_and_Non-uniform_quantization">code</a>  -->
                  <br>
              </div>
                <p>
                  We will demonstrate a real-time implementation of a protoobject based neuromorphic visual saliency model on an embedded processing board.
                  Visual saliency models are difficult to implement in hardware for real-time applications due to their computational complexity.
                  The conventional implementation is not optimal because of the requirement of a large number of convolution operations for filtering on several feature channels across multiple image pyramids. Our current implementation considers the dynamic temporal motion change by convoluting along time efficiently by parallelly processing them. 
                </p>
              
                <br>
            </td>
        </tr>
      </td>
    </tr>

    <tr>
        <td style="padding:20px;width:35%;vertical-align:middle">
            <img src='images/cs.png' width="250"></div>
                </td>
        <td width="75%" valign="middle">
            <p>
          
                <b>A Compressive Sensing Video dataset using Pixel-wise coded exposure</b> <br>
     
          
            
            <i><b>Sathyaprakash Narayanan</b>,
              Yeshwanth Ravi Theja,
              Chetan Singh Thakur</i>
                  
            <i><b>arXiv, 2019</b></i> <br>
            
            </p>
            <div class="paper" id="btp">
              <a href="https://arxiv.org/pdf/1905.10054.pdf">Paper</a> 
              <!-- <br> -->
              
          </div>

            <p>
              Manifold amount of video data gets generated every minute as we read this document, ranging from surveillance to broadcasting purposes. 
              There are two roadblocks that restrain us from using this data as such, first being the storage which restricts us from only storing the information based on the hardware constraints.
              Secondly, the computation required to process this data is highly expensive which makes it infeasible to work on them.
              Compressive sensing(CS) is a signal process technique, through optimization, the sparsity of a signal can be exploited to recover it from far fewer samples than required by the Shannon-Nyquist sampling theorem.
              There are two conditions under which recovery is possible. In this work we propose a new comprehensive video dataset, where the data is compressed using pixel-wise coded exposure that resolves various other impediments.
            </p>
            
            <br>
        </td>
    </tr>
  </td>
</tr>
  </table>


<!-- 



  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <td style="padding:10px;width:100%;vertical-align:middle">
      <heading>Patent</heading>
    </td>
  </tr>
  <!-- <h2 class="card-subheading">Paper</h2> -->
<!-- </tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


  <tr>
    <td style="padding:20px;width:35%;vertical-align:middle">
        <img src='images/near.png' height="120" width="250"></div>
            </td>
    <td width="75%" valign="middle">
        <p>
      
            <b>Method and system for recognizing activities in surrounding environment for controlling navigation of autonomous vehicle</b>
        
            <i><b>Sathyaprakash Narayanan</b>,
              Yeshwanth Ravi Theja,
              Bibrat Ranjan Pradhan,
              Anirban Charkraborty,
              Chetan Singh Thakur</i>
             
        <br>
        <div class="paper" id="btp">
          <a href="https://patents.google.com/patent/US20220324477A1/en">Patent</a>
 
          
      </div>
        </p>
        <p>
          A method and activity recognition system for recognising activities in surrounding environment for controlling navigation of an autonomous vehicle is disclosed. The activity recognition system receives first data feed from neuromorphic event-based camera and second data feed from frame-based RGB video camera. 
        </p>
      
        <br>
    </td>
</tr>

 
</table> --> 


  <hr>
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>

          <tr>
            <td>
              <heading>Responsibilities</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<img src="images/sISCAS-2024.png" alt="spi">
            </td>
            <td width="75%" valign="middle">
              <a href="https://2024.ieee-iscas.org/">Reviewer <b>ISCAS</b> 2024</a>
      
            </td>
          </tr>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, <b>WACV</b> 2019, 2020, 2022, 2023, 2024</a>
              <br>
              <a href="http://cvpr2021.thecvf.com/area-chairs">Reviewer, <b>BMVC</b> 2018</a>
              <br>
              
              
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rsz_download.png" alt="tpami">
            </td>
            <td width="75%" valign="center">
              <a href="http://cvpr2019.thecvf.com/area_chairs">Reviewer, <b>TPAMI</b> IEEE Transactions on Pattern Analysis and Machine Intelligence</a>
              <br>

            </td>
          </tr>
					<!-- sip-logo.jpg -->

          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<img src="images/rsz_1sip-logo.jpg" alt="spi">
            </td>
            <td width="75%" valign="middle">
              <a href="https://sip.ucsc.edu/">SIP Mentor 2023</a>
      
            </td>
          </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Forked from <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
