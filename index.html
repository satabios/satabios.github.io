<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sathyaprakash Narayanan</title>
  
  <meta name="author" content="Sathyaprakash Narayanan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/x-icon" href="/images/icon-48x48.png">
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-KFDSH0REN5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-KFDSH0REN5');
</script>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sathyaprakash Narayanan</name>
              </p>
            
              <p>Hi there! I am an AI Researcher broadly interested in Deep Learning and Neuromorphic Engineering.</p>

              <p>Currently, making AI model effficient and lightweight at <a href="https://www.qualcomm.com/">Qualcomm</a> as <b>AI Systems Engineer</b>.</p>

              <p>
                  Graduated from the <a href="https://www.ucsc.edu/">University of California, Santa Cruz</a>.
                  <br><br>
                  In the meantime, I single-handedly authored and developed <a href="https://github.com/satabios/sconce"><b>sconce</b></a>, an end-to-end AutoDL model compression and inference package under the supervision of Professor and Mentor, <a href="https://www.jasoneshraghian.com/">Jason Eshraghian</a>.
                  <br><br>
                  Before that, I worked as a <b>Machine Learning Scientist II</b> at <a href="https://www.lytx.com/en-us/">Lytx Inc.</a>.
                  <br><br>
                  Prior to that, I was a <b>Research Associate</b> at the <a href="https://labs.dese.iisc.ac.in/neuronics/">NeuRonICS Lab, DESE, Indian Institute of Science, Bangalore</a>, and a <b>Teaching Assistant</b> at <a href="https://www.mygreatlearning.com/">Great Learning, Bangalore</a>.
                  <br><br>
              </p>


                <!-- <br> 
                Teaching at UCSC:
                <ul>
                  <li><strong>Group Tutor</strong>: CSE114A (Sep 2022 - Dec 2022)</li>
                  <li><strong>Reader</strong>: CSE 142 (Sep 2022 - Dec 2022)</li>
                  <li><strong>Teaching Assistant</strong>: CSE 30-01 - Programming Abstractions: Python (Jan 2023 - Mar 2023)</li>
                  <li><strong>Teaching Assistant</strong>: ECE 103 - Signals and Systems (Apr 2023 - Jun 2023)</li>
                  <li><strong>Reader</strong>: STAT 131-02 (Jul 2023 - Sep 2023)</li>
                  <li><strong>SIP Mentor</strong>: Science Internship Program (Aug 2023 - Sep 2023)</li>
                  <li><strong>Teaching Assistant</strong>: TIM 170 - Management of Technology (Sep 2023 - Dec 2023)</li>
                  <li><strong>Teaching Assistant</strong>: TIM 170 - Management of Technology (Jan 2024 - Mar 2024)</li>
                  <li><strong>Teaching Assistant</strong>: TIM 170 - Management of Technology (Apr 2024 - Jun 2024)</li>
              </ul>
               -->

                <!-- <br>  -->
     
              </p>
              <p style="text-align:center">
                <a href="data/resume.pdf">CV</a>&nbsp/&nbsp
                <!-- <a href="mailto:1f681as95@mozmail.com">Email</a>&nbsp/&nbsp -->
                <a href="https://github.com/satabios/">Github</a>&nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=rI5VHWoAAAAJ">Google Scholar</a>&nbsp/&nbsp
                <a href="https://www.linkedin.com/in/sathyaprakash-narayanan-87b07850/">LinkedIn</a>&nbsp/&nbsp
                <a href="https://www.kaggle.com/satabios">Kaggle</a>&nbsp/&nbsp                
                <a href="https://twitter.com/sathyabratrat">Twitter</a>
                
                
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/IMG_1467."><img style="width:100%;max-width:100%" alt="profile photo" src="images/IMG_1467.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Interest</heading>
              <p>
                My research interests broadly are and but is not restricted to:
                <ul>
                <li>Deep Learning</li>
                <li>Machine Learning</li>
                <li>Computer Vision</li>
                <li>Efficient ML/DL Model Hardware Deployment</li>
                <li>Computational Imaging</li>
                <li>Signal Processing</li>
                <li>Compressive Sensing</li>
                <li>Neuromorphic Engineering</li>
                <!-- <li>Event Based Data</li> -->
                </ul>
              
              <!-- <span class="highlight">highlighted</span>. -->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
          
          <hr>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
                <heading>Recent Update</heading>
            </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <ul>
          <!-- <li><strong>[March 2025]</strong> Joining  <b>Qualcomm</b> as AI Systems Engineer</li> -->
          <li><strong>[Jan, 2026]</strong> Built Fastron(Standalone) Application Released <b>fastron: __version__= "Central"</b></li>
          <li><strong>[Aug, 2025]</strong> Built Fastron(vscode Extension)</li>
          <li><strong>[July, 2025]</strong> Built Qtron(vscode Extension)</li>
          <li><strong>[July, 2025]</strong> Built ONNX Comparator(vscode Extension)</li>
          <li><strong>[Jun, 2024]</strong> Selected as a Reviewer for <b>WACV 2025</b></li>
          <li><strong>[Feb, 2024]</strong> Second Version of sconce: model compression/inference package Released <b>sconce: __version__= "Spider"</b></li>
          <li><strong>[Jan, 2024]</strong> First Version of sconce: model compression/inference package Released <b>sconce: __version__= "Ant"</b></li>
          <li><strong>[Jun, 2023]</strong> Selected as a Reviewer for <b>WACV 2024</b></li>
          <li><strong>[May, 2023]</strong> Selected as a <b>SIP 2023 Mentor</b></li>
          <li><strong>[Apr, 2023]</strong> Pushed the first version of <b>Model Compression(tomoco)</b> </li>
          <li><strong>[Jan, 2023]</strong> Selected as a Reviewer for <b>WACV 2023</b></li>
          <li><strong>[Spring 2023]</strong> <b>ECE103</b>: Teaching Assistant for Signals and Systems Coursework at UCSC</li>
          <li><strong>[Winter 2023]</strong> <b>CSE30</b>: Teaching Assistant for  Programming Abstractions: Python at UCSC</li>
          <li><strong>[Dec, 2021]</strong> <b>Patent Published</b>: <i>METHOD AND SYSTEM FOR RECOGNIZING ACTIVITIES IN SURROUNDING ENVIRONMENT FOR CONTROLLING NAVIGATION OF AUTONOMOUS VEHICLE</i></li>
          <li><strong>[June, 2021]</strong> Paper Selected for <b>ICIP 2021</b></li>
          
          
        </ul>
      </tr>
      </tbody></table>


          <hr>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Making Life a bit Easier for Myself!</heading>
            </td>
          </tr>
        </tbody></table>
        
        <!-- VS Code Extensions -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src='https://raw.githubusercontent.com/satabios/Fastron/experiment/source/icon.png' width="120" height="120" alt="Fastron">
            </td>
            <td width="75%" valign="middle">
              <p>
                <b>Fastron - Optimized Neural Network Visualizer</b><br>
              </p>
              <div class="paper" id="fastron">
                <a href="https://marketplace.visualstudio.com/items?itemName=satabios.fastron">VS Code Marketplace</a> / 
                <a href="https://github.com/satabios/Fastron/">Github</a>
              </div>
              <p>
                Fastron: A performance‑tuned Netron that enables ultra‑fast, memory‑efficient visualization of large machine learning models.
              </p>
              <br>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src='https://github.com/satabios/onnx-comparator/blob/main/images/Gemini_Generated_Image_kstdnrkstdnrkstd.png?raw=true' width="120" height="120" alt="ONNX Model Comparator">
            </td>
            <td width="75%" valign="middle">
              <p>
                <b>ONNX Model Comparator</b><br>
                <i>VS Code Extension</i>
              </p>
              <div class="paper" id="onnx-comparator">
                <a href="https://marketplace.visualstudio.com/items?itemName=satabios.onnx-model-comparator">VS Code Marketplace</a>
              </div>
              <p>
                A Visual Studio Code extension for comparing ONNX graphs, providing insights into model differences and analysis.
              </p>
              <br>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src='https://github.com/satabios/qtron/blob/main/images/onnxviewer_128.png?raw=true' width="120" height="120" alt="Qtron">
            </td>
            <td width="75%" valign="middle">
              <p>
                <b>Qtron</b><br>
                <i>VS Code Extension</i>
              </p>
              <div class="paper" id="qtron">
                <a href="https://marketplace.visualstudio.com/items?itemName=satabios.qtron">VS Code Marketplace</a>
              </div>
              <p>
               A lightweight VS Code extension for viewing and analyzing ONNX models with seamless onnx_tool integration.
              </p>
              <br>
            </td>
          </tr>
        </tbody></table>

        <hr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Works</heading>
            </td>
          </tr>
          <!-- <h2 class="card-subheading">Paper</h2> -->
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
            <!-- <tr bgcolor="#ffffd0"> -->
        <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/sconce-punch-bk_removed.png' width="250"></div>
              </td>
      <td width="75%" valign="middle">
          <p>
         
              <b>sconce: E2E AutoML Model Compression/Inference Package</b><br>
              <i>
                Sathyaprakash Narayanan
                </i>
              
          </a>
          

          </p>
          <div class="paper" id="btp">
            <a href="https://github.com/satabios/sconce">Github</a>/
            <a href="https://sconce.readthedocs.io/en/latest/">Docs</a>
            <br>
        </div>
          <p>
	 A one-stop solution for Model Compression, from NAS, Pruning, Quantization, Layer Fusion, Sparse Engine to CUDA optimizations for Inference Optimizations.
	  </p>
          
          <br>
      </td>
  </tr>

          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/compressive_sensing_DL.png' height="120" width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
              
                    <b>Real-Time Object Detection and Localization in Compressive Sensed Video</b>
                
                    <i><strong>Sathyaprakash Narayanan</strong>*,
                      Yeshwanth Ravi Theja*,
                      Venkat Rangan,
                      Anirban Charkraborty,
                      Chetan Singh Thakur</i><br>
                <i>IEEE International Conference on Image Processing <b>(ICIP), 2021</b></i>
                <br>
                <div class="paper" id="btp">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9506769">Paper</a>/ 
                  <a href="https://labs.dese.iisc.ac.in/neuronics/datasets/">Dataset</a> 
                  
              </div>
                </p>
                <p>
                  A novel task of detection and localization of objects directly
                  on the compressed frames. Thereby mitigating the need to
                  reconstruct the frames and reducing the search rate up to
                  20x (compression rate). We achieved an accuracy of 46.27%
                  mAP with the proposed model. We were also able to show real-time inference on an NVIDIA
                  TX2 embedded board with 45.11% mAP.
                </p>
              
                <br>
            </td>
        </tr>

          <!-- <tr bgcolor="#ffffd0"> -->
        <td style="padding:20px;width:35%;vertical-align:middle">
            <img src='images/n-har.png' width="250"></div>
                </td>
        <td width="75%" valign="middle">
            <p>
           
                <b>n-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory Surfaces</b><br>
                <i>Bibrat Ranjan Pradhan,
                  Yeshwanth Ravi Theja,
                  <b>Sathyaprakash Narayanan</b>,
                  Anirban Charkraborty,
                  Chetan Singh Thakur</i><br>
                  <i>IEEE International Symposium on Circuits and Systems <b>(ISCAS), 2019</b></i>
                
            </a>
            

            </p>
            <div class="paper" id="btp">
              <a href="https://ieeexplore.ieee.org/abstract/document/8702581">Paper</a>/
              <a href="https://labs.dese.iisc.ac.in/neuronics/datasets/event-based-neuromorphic-datasets/">Code-Dataset</a>
              <br>
          </div>
            <p>
              A system to achieve the task of human activity recognition based on the event-based camera data. 
              We show that such tasks, which generally need high frame rate sensors for accurate predictions, can be achieved by adapting existing computer vision techniques to the spiking domain.
               We used event memory surfaces to make the sparse event data compatible with deep convolutional neural networks (CNNs).
                <!-- We leverage upon the recent advances in deep convolutional networks based video analysis and adapt such frameworks onto the neuromorphic domain. -->
                 We also provide the community with a new dataset consisting of five categories of human activities captured in real world without any simulations.
                  We achieved an accuracy of 94.3% using event memory surfaces on our activity recognition dataset.
            </p>
            
            <br>
        </td>
    </tr>

  

        <!-- <tr bgcolor="#ffffd0"> -->
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/saliency.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <!-- <p><strong>Paper: </strong> -->
                
                    <b>Real-Time Implementation of Proto-Object Based Visual Saliency Model</b><br>
                    <i><b>Sathyaprakash Narayanan</b>,
                      Yeshwanth Ravi Theja,
                      Jamal Lottier,
                      Ernst Niebur,
                      Ralph Etienne-Cummings,
                      Chetan Singh Thakur</i> <br>
                <i>IEEE International Symposium on Circuits and Systems <b>(ISCAS), 2019</b></i>
                </p>

                <div class="paper" id="ijcnn_2021">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8702200">Paper</a> 
                  <!-- <a href="https://github.com/raja-kumar/Hybrid_and_Non-uniform_quantization">code</a>  -->
                  <br>
              </div>
                <p>
                  We will demonstrate a real-time implementation of a protoobject based neuromorphic visual saliency model on an embedded processing board.
                  Visual saliency models are difficult to implement in hardware for real-time applications due to their computational complexity.
                  The conventional implementation is not optimal because of the requirement of a large number of convolution operations for filtering on several feature channels across multiple image pyramids. Our current implementation considers the dynamic temporal motion change by convoluting along time efficiently by parallelly processing them. 
                </p>
              
                <br>
            </td>
        </tr>
      </td>
    </tr>

    <tr>
        <td style="padding:20px;width:35%;vertical-align:middle">
            <img src='images/cs.png' width="250"></div>
                </td>
        <td width="75%" valign="middle">
            <p>
          
                <b>A Compressive Sensing Video dataset using Pixel-wise coded exposure</b> <br>
     
          
            
            <i><b>Sathyaprakash Narayanan</b>,
              Yeshwanth Ravi Theja,
              Chetan Singh Thakur</i>
                  
            <i><b>arXiv, 2019</b></i> <br>
            
            </p>
            <div class="paper" id="btp">
              <a href="https://arxiv.org/pdf/1905.10054.pdf">Paper</a> 
              <!-- <br> -->
              
          </div>

            <p>
              Manifold amount of video data gets generated every minute as we read this document, ranging from surveillance to broadcasting purposes. 
              There are two roadblocks that restrain us from using this data as such, first being the storage which restricts us from only storing the information based on the hardware constraints.
              Secondly, the computation required to process this data is highly expensive which makes it infeasible to work on them.
              Compressive sensing(CS) is a signal process technique, through optimization, the sparsity of a signal can be exploited to recover it from far fewer samples than required by the Shannon-Nyquist sampling theorem.
              There are two conditions under which recovery is possible. In this work we propose a new comprehensive video dataset, where the data is compressed using pixel-wise coded exposure that resolves various other impediments.
            </p>
            
            <br>
        </td>
    </tr>
  </td>
</tr>
  </table>


<!-- 



  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <td style="padding:10px;width:100%;vertical-align:middle">
      <heading>Patent</heading>
    </td>
  </tr>
  <!-- <h2 class="card-subheading">Paper</h2> -->
<!-- </tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


  <tr>
    <td style="padding:20px;width:35%;vertical-align:middle">
        <img src='images/near.png' height="120" width="250"></div>
            </td>
    <td width="75%" valign="middle">
        <p>
      
            <b>Method and system for recognizing activities in surrounding environment for controlling navigation of autonomous vehicle</b>
        
            <i><b>Sathyaprakash Narayanan</b>,
              Yeshwanth Ravi Theja,
              Bibrat Ranjan Pradhan,
              Anirban Charkraborty,
              Chetan Singh Thakur</i>
             
        <br>
        <div class="paper" id="btp">
          <a href="https://patents.google.com/patent/US20220324477A1/en">Patent</a>
 
          
      </div>
        </p>
        <p>
          A method and activity recognition system for recognising activities in surrounding environment for controlling navigation of an autonomous vehicle is disclosed. The activity recognition system receives first data feed from neuromorphic event-based camera and second data feed from frame-based RGB video camera. 
        </p>
      
        <br>
    </td>
</tr>

 
</table> --> 


  <hr>
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>

          <tr>
            <td>
              <heading>Responsibilities</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<img src="images/sISCAS-2024.png" alt="spi">
            </td>
            <td width="75%" valign="middle">
              <a href="https://2024.ieee-iscas.org/">Reviewer <b>ISCAS</b> 2024</a>
      
            </td>
          </tr>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, <b>WACV</b> 2019, 2020, 2022, 2023, 2024</a>
              <br>
              <a href="http://cvpr2021.thecvf.com/area-chairs">Reviewer, <b>BMVC</b> 2018</a>
              <br>
              
              
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rsz_download.png" alt="tpami">
            </td>
            <td width="75%" valign="center">
              <a href="http://cvpr2019.thecvf.com/area_chairs">Reviewer, <b>TPAMI</b> IEEE Transactions on Pattern Analysis and Machine Intelligence</a>
              <br>

            </td>
          </tr>
					<!-- sip-logo.jpg -->

          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<img src="images/rsz_1sip-logo.jpg" alt="spi">
            </td>
            <td width="75%" valign="middle">
              <a href="https://sip.ucsc.edu/">SIP Mentor 2023</a>
      
            </td>
          </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Forked from <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>

